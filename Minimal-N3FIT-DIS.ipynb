{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fad122",
   "metadata": {},
   "source": [
    "# A minimal version of N3FIT-DIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b41b2",
   "metadata": {},
   "source": [
    "The following notebook minimally implements what **n3fit** does (mainly DIS). For the sake of simplicity, we use as inputs saved files (`toyexpinfo.pkl` for experimental datasets, `posdatasets.pkl` for positivity datasets, and `integdatasets.pkl` for integrability datasets) generated from a **n3fit** run using the `toy-runcard.yaml`. In the current NNPDF machinery, the loading and parsing of the inputs are handled by **validphys**. Here, the only part that we care about is to illustrate how theoretical predictions are produced upon fitting the PDFs with Neural Networks and subsequently how are they compared to the experimental measurements. \n",
    "\n",
    "This notebook therefore tries to accomplishes two purposes: (1) provide an easy introduction to the fitting machinery of the NNPDF code by simplifying the code, (2) serve as a baseline for experimenting new and particularly subtle features. In these regards, the classes, methods, and attributes defined below are mirrored from those appearing in **n3fit** in order to trivially make a one to one compraison. Three disclaimers: \n",
    "* **n3fit** is much more modular and makes various calls from **validphys** (which makes the structure of codes complicated) and hence things might be slightly represented differently there.\n",
    "*  Some of the advanced features such as **k-folding**, **hyperparameter optimization**, and **feature scaling** are not touched here.\n",
    "* To facilitate various operations we are going to use the the **n3fit** backends instead of the keras backends.The **n3fit** backends provide extra-flexibility that comes very handy, one example is the ability to fix the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b983a",
   "metadata": {},
   "source": [
    "## 1. Definitions and notations\n",
    "\n",
    "For **Deep Inelastic Scattering (DIS)**, observables are represented as a matrix multiplication between an FK-table\n",
    "and PDFs as follows:\n",
    "$$ \\mathcal{O}^{\\rm th}_d = \\sum^{N_x}_{\\alpha} \\sum^{N_f}_{i} \\mathrm{FK}_{\\alpha i}^{(d)} \\mathrm{NN}_i (x_\\alpha, Q_0), $$\n",
    "where $\\alpha$ and $i$ denote the data point and the $x$ index respectively. $\\mathrm{NN}_i (x_\\alpha, Q_0)$ represents the PDF in the **evolution basis** evaluated at the initial scale $Q_0$ and at a $x$-grid point $x = x_\\alpha$. $\\mathrm{FK}_{\\alpha i}^{(d)}$ denotes the FK table corresponding to $x = x_\\alpha$ in $x$-grid for a data point $d$ and associated with the PDF $i$. Here, it is important to emphasize that every dataset contains different number of $x$ and data points. For a detaled information on how FK tables are formated in the actual NNPDF fit, refer to this [documentation](https://docs.nnpdf.science/data/th-data-files.html).\n",
    "\n",
    "| Notation      | Default | Description                |\n",
    "|---------------|---------|----------------------------|\n",
    "| $N_f$         | 14      | PDF basis                  |\n",
    "| $N_x$         | None    | Size of $x$-grid           |\n",
    "| $N_{\\rm dpt}$ | None    | Total number of datapoints |\n",
    "\n",
    "Following the **n3fit** and **validphys** implementations, the inputs to the fit have the following shapes:\n",
    "\n",
    "| Data                       | Description    | shape                         |\n",
    "|----------------------------|----------------|-------------------------------|\n",
    "| $\\mathrm{FK}$              | rank-3 tensor  | ($N_{\\rm dpt}$, $N_f$, $N_x$) |\n",
    "| $\\mathcal{O}$$^{\\rm exp}$  | 1-d array      | ($N_{\\rm dpt}$)               |\n",
    "| $\\mathrm{cov}$             | Square matrix  | ($N_{\\rm dpt}$, $N_{\\rm dpt}$)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ea08b",
   "metadata": {},
   "source": [
    "## 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ec2d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras backend\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "from dataclasses import dataclass\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Use n3fit backends which are wrappers around\n",
    "# tf.keras backends.\n",
    "from n3fit.backends import Input\n",
    "from n3fit.backends import base_layer_selector\n",
    "from n3fit.backends import MetaModel\n",
    "from n3fit.backends import callbacks\n",
    "from n3fit.backends import MetaLayer\n",
    "from n3fit.backends import operations as op\n",
    "from n3fit.backends import clear_backend_state\n",
    "from n3fit.stopping import Stopping\n",
    "\n",
    "# Define seeds\n",
    "random.seed(123)\n",
    "np.random.seed(456)\n",
    "console = Console()\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d2eec",
   "metadata": {},
   "source": [
    "## 3. Load saved toy-datasets from N3FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3309de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Nb. Datasets: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mNb. Datasets: \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_pkl_file = open(\"toyexpinfo.pkl\", \"rb\")\n",
    "toyexpinfo = pickle.load(exp_pkl_file)\n",
    "console.print(f\"Nb. Datasets: {len(toyexpinfo[0]['datasets'])}\", style=\"bold blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd59e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pkl_file = open(\"posdatasets.pkl\", \"rb\")\n",
    "toyposdatasets = pickle.load(pos_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a35dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "integ_pkl_file = open(\"integdatasets.pkl\", \"rb\")\n",
    "toyintegdatasets = pickle.load(integ_pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd64dd",
   "metadata": {},
   "source": [
    "## 4. Create the NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b66f1",
   "metadata": {},
   "source": [
    "### 4.1 Construct the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cac4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_network(\n",
    "    nodes_in,\n",
    "    nodes,\n",
    "    activations,\n",
    "    initializer_name=\"glorot_normal\",\n",
    "    seed=0,\n",
    "    dropout_rate=0.0,\n",
    "    regularizer=None,\n",
    "):\n",
    "    \"\"\"This function generate the different `tf.keras.layers.Dense` layers and add\n",
    "    them to a list. The number of layers depends on the length of the `node_per_layer`.\n",
    "    This function mimicks the behaviour of the `generate_dense_network` function in \n",
    "    the `n3fit.model_gen` module.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list:\n",
    "        List of `tf.keras.layers.Dense` layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    choose_keras_layer_type = \"dense\"\n",
    "    list_of_layers = []\n",
    "    for (nodes_out, activation) in zip(nodes, activations):\n",
    "        list_of_layers.append(base_layer_selector(\n",
    "            choose_keras_layer_type,\n",
    "            units=int(nodes_out),\n",
    "            input_shape=(nodes_in,),\n",
    "            kernel_initializer=initializer_name,\n",
    "            activation=activation\n",
    "        ))\n",
    "        # Make sure that the next layer has the same input nodes as the\n",
    "        # output of the previous layer\n",
    "        nodes_in = int(nodes_out)\n",
    "    return list_of_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f449",
   "metadata": {},
   "source": [
    "### 4.2 Construct the complete NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbaf480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n3fit.layers import DIS, DY, ObsRotation, losses\n",
    "from n3fit.layers import Preprocessing, FkRotation, FlavourToEvolution\n",
    "from n3fit.backends import MetaLayer, Lambda\n",
    "from n3fit.backends import base_layer_selector, regularizer_selector\n",
    "\n",
    "def pdfNN_layer_generator(\n",
    "    inp=2,\n",
    "    nodes=None,\n",
    "    activations=None,\n",
    "    initializer_name=\"glorot_normal\",\n",
    "    layer_type=\"dense\",\n",
    "    flav_info=None,\n",
    "    fitbasis=\"NN31IC\",\n",
    "    out=14,\n",
    "    seed=None,\n",
    "    dropout=0.0,\n",
    "    regularizer=None,\n",
    "    regularizer_args=None,\n",
    "    impose_sumrule=None,\n",
    "    scaler=None,\n",
    "    parallel_models=1,\n",
    "):\n",
    "    \"\"\"This function constructs the main Neural Network to fit the PDFs. This part\n",
    "    mimicks the function `pdfNN_layer_generator` present in `n3fit.model_gen` module\n",
    "    by simplifying it greatly, i.e. removing all the details such as Evolution,\n",
    "    basis rotation, and sum rules. Notice that these layers are not trained during\n",
    "    the fitting procedure and therefore are immaterial here.\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_layers = len(nodes)\n",
    "    # Last layer fixed to be fl=14 (flavour basis)\n",
    "    last_layer_nodes = nodes[-1]\n",
    "\n",
    "    # First prepare the input for the PDF model (no scaling)\n",
    "    placeholder_input = Input(shape=(None, 1), batch_size=1)\n",
    "    \n",
    "    process_input = Lambda(lambda x: op.concatenate([x, op.op_log(x)], axis=-1))\n",
    "\n",
    "    model_input = [placeholder_input]\n",
    "\n",
    "    layer_evln = lambda x: x       # Mimick Evolution\n",
    "    basis_rotation = lambda x: x   # Mimick Basis Rotation\n",
    "    sumrule_layer = lambda x: x    # Mimick Sum Rules\n",
    "\n",
    "    pdf_models = []\n",
    "    for i, layer_seed in enumerate(seed):\n",
    "        list_of_pdf_layers = generate_dense_network(\n",
    "            inp,\n",
    "            nodes,\n",
    "            activations,\n",
    "            initializer_name,\n",
    "            seed=layer_seed,\n",
    "            dropout_rate=dropout,\n",
    "            regularizer=None,\n",
    "        )\n",
    "\n",
    "        def dense_me(x):\n",
    "            \"\"\"Function that takes a list of layers and connect them.\"\"\"\n",
    "            processed_x = process_input(x)\n",
    "            curr_fun = list_of_pdf_layers[0](processed_x)\n",
    "\n",
    "            for dense_layer in list_of_pdf_layers[1:]:\n",
    "                curr_fun = dense_layer(curr_fun)\n",
    "            return curr_fun\n",
    "\n",
    "        def layer_fitbasis(x):\n",
    "            \"\"\"Funccation that performs the roation to change the basis.\"\"\"\n",
    "            x_scaled = op.op_gather_keep_dims(x, 0, axis=-1)\n",
    "            x_original = op.op_gather_keep_dims(x, -1, axis=-1)\n",
    "            nn_output = dense_me(x_scaled)\n",
    "            return basis_rotation(nn_output)\n",
    "\n",
    "        def layer_pdf(x):\n",
    "            return layer_evln(layer_fitbasis(x))\n",
    "\n",
    "        # Final PDF (apply normalization)\n",
    "        final_pdf = sumrule_layer(layer_pdf)\n",
    "\n",
    "        # Create the model\n",
    "        pdf_model = MetaModel(model_input, final_pdf(placeholder_input), name=f\"PDF_{i}\", scaler=scaler)\n",
    "        pdf_models.append(pdf_model)\n",
    "    return pdf_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725358c4",
   "metadata": {},
   "source": [
    "### 4.3 Construct the Observable\n",
    "\n",
    "Now, we can implement the part that computes the Observable ($\\mathcal{O}^{\\rm th}$) expressed in the equation above. Due to the way how the observables are computed, the standard *tf.keras.layers* will no longer work, instead, one has to define a custom layer which inherits from the *tf.keras.layers.Layer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5ffd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_unique(list_of_arrays):\n",
    "    \"\"\"This function takes a list of numpy arrays and checks whether it contains\n",
    "    more tha one different arrays.\n",
    "    \"\"\"\n",
    "    the_first = list_of_arrays[0]\n",
    "    for i in list_of_arrays[1:]:\n",
    "        if not np.array_equal(the_first, i):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class Observable(MetaLayer, ABC):\n",
    "    \"\"\"Abstract classs that initializes the relevant inputs for the computation\n",
    "    of the theoretical predictions. This Class does not yet have a method that\n",
    "    actually computes the Observable.\n",
    "    \n",
    "    This class is an almost exact copy of the class `Observable` contained in the\n",
    "    module `n3fit.layers.observable`.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    list_fktable_dicts: list\n",
    "        list of dictionaries where each element contains the specifications about\n",
    "        a given FK table. Each dictionary has the following keys:\n",
    "        - ndata: nb of datapoints\n",
    "        - nbasis:\n",
    "        - nonzero: nb o non-zero PDF entries\n",
    "        - basis: gives the index of the non-zero PDFs\n",
    "        - nx: size of the x-grid\n",
    "        - xgrid: array of x-points\n",
    "        - fktable: full/unmasked FK table of shape (ndata, nonzero, nx)\n",
    "            \n",
    "    list_fktable_mask: list of boolean arrays\n",
    "        list of training/validation mask to be applied to the fktable/observable\n",
    "        \n",
    "    operation_name: str\n",
    "        operation to be performed for a given FK table(s)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fktable_dicts, operation_name, nfl=14, **kwargs):\n",
    "        \n",
    "        super(MetaLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.nfl = nfl\n",
    "        basis, xgrids, self.fktables = [], [], []\n",
    "\n",
    "        for fktable in fktable_dicts:\n",
    "            xgrids.append(fktable[\"xgrid\"])\n",
    "            basis.append(fktable[\"basis\"])\n",
    "            self.fktables.append(fktable[\"fktable\"])\n",
    "            \n",
    "        # check how many xgrids this dataset needs\n",
    "        if _is_unique(xgrids): self.splitting = None\n",
    "        else: self.splitting = [i.shape[1] for i in xgrids]\n",
    "\n",
    "        # check how many basis this dataset needs\n",
    "        if _is_unique(basis) and _is_unique(xgrids):\n",
    "            self.all_masks = [self.gen_mask(basis[0])]\n",
    "            self.many_masks = False\n",
    "        else:\n",
    "            self.many_masks = True\n",
    "            self.all_masks = [self.gen_mask(i) for i in basis]\n",
    "\n",
    "        self.operation = op.c_to_py_fun(operation_name)\n",
    "        self.output_dim = self.fktables[0].shape[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (self.output_dim, None)\n",
    "\n",
    "    @abstractmethod\n",
    "    def gen_mask(self, basis):\n",
    "        \"\"\"This method needs to be initialized here in order for it to get\n",
    "        overwritten when it is called elsewhere.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea119d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIS(Observable):\n",
    "    \"\"\"Class that inherits from the Observable Layer class. It fetches all the relevant\n",
    "    variables from the parent class, overwrites the masks, and compute the theory predictions\n",
    "    using the formula written above. This class is an almost exact copy of the `DIS` class\n",
    "    in the `n3fit.layers` module.\n",
    "    \"\"\"\n",
    "\n",
    "    def gen_mask(self, basis):\n",
    "        \"\"\"Takes a list of the index of the non-zero PDFs and return a boolean mask.\"\"\"\n",
    "        \n",
    "        if basis is None:\n",
    "            self.basis = np.ones(self.nfl, dtype=bool)\n",
    "        else:\n",
    "            basis_mask = np.zeros(self.nfl, dtype=bool)\n",
    "            for i in basis:\n",
    "                basis_mask[i] = True\n",
    "        return op.numpy_to_tensor(basis_mask, dtype=bool)\n",
    "    \n",
    "\n",
    "    def call(self, pdf):\n",
    "        \"\"\"Performs the convolution between an FK table with a PDF.\n",
    "        \n",
    "        Ingredients:\n",
    "        ------------\n",
    "        self.fktables: list\n",
    "            list of masked FK tables (for training) where each FK tables has a shape \n",
    "            (ndata_masked, non_zero_fl, nx)\n",
    "            \n",
    "        pdf: list\n",
    "            list of PDF tensor (output of the NN) with shape (batch_size, nx, non_zero_fl)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.splitting is not None:\n",
    "            raise ValueError(\"Splitting should always be None for DIS.\")\n",
    "\n",
    "        results = []\n",
    "        if self.many_masks:\n",
    "            for mask, fktable in zip(self.all_masks, self.fktables):\n",
    "                # We also need to mask the PDFs output from the NN. Recall that \n",
    "                # the output of the NN is the 14 PDFs in the flavour basis\n",
    "                pdf_masked = op.boolean_mask(pdf, mask, axis=2)\n",
    "                res = op.tensor_product(pdf_masked, fktable, axes=[(1, 2), (2, 1)])\n",
    "                results.append(res)\n",
    "        else:\n",
    "            pdf_masked = op.boolean_mask(pdf, self.all_masks[0], axis=2)\n",
    "            for fktable in self.fktables:\n",
    "                res = op.tensor_product(pdf_masked, fktable, axes=[(1, 2), (2, 1)])\n",
    "                results.append(res)\n",
    "        \n",
    "        res = self.operation(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fbab62",
   "metadata": {},
   "source": [
    "As in the case of the PDFs `pdfNN_layer_generator`, we need an observable generator. The way **n3fit** this is to first define an `Observable` wrapper that constructs the graph for the computation of the **loss functions**. Here, since we are not yet interested in the form of the loss functions, we are just going to rely on the `losses` module of **n3fit**. In particular, we are only going to use the `LossInvcovmat` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8939ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n3fit.layers.losses import LossPositivity\n",
    "from n3fit.layers.losses import LossInvcovmat\n",
    "from n3fit.layers.losses import LossIntegrability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6939372",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ObservableWrapper:\n",
    "    \"\"\"This class mimicks the `ObservableWrapper` class in the module `n3fit.model_gen`.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    observables: list\n",
    "    masks: list\n",
    "    dataset_xsizes: list\n",
    "    invcovmat: np.array = None\n",
    "    covmat: np.array = None\n",
    "    multiplier: float = 1.0\n",
    "    integrability: bool = False\n",
    "    positivity: bool = False\n",
    "    data: np.array = None\n",
    "    rotation: ObsRotation = None\n",
    "\n",
    "    def _generate_loss(self, mask=None):\n",
    "        \"\"\"Returns `n3fit.backends.layer.MetaLayer` of the loss function.\"\"\"\n",
    "        \n",
    "        if self.invcovmat is not None:\n",
    "            for d, m in zip(self.data, self.masks):\n",
    "                console.log(\"The incovmat input data is now\", d.shape)\n",
    "            if m is not None:\n",
    "                console.log(\"The input trvl mask is now\", m.mask.shape)\n",
    "            #mask = self.masks[0].mask if self.masks[0] is not None else None\n",
    "            loss = losses.LossInvcovmat(self.invcovmat, self.data, mask, name=self.name)\n",
    "        elif self.positivity:\n",
    "            loss = losses.LossPositivity(name=self.name, c=self.multiplier)\n",
    "        elif self.integrability:\n",
    "            loss = losses.LossIntegrability(name=self.name, c=self.multiplier)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def _generate_experimental_layer(self, pdf):\n",
    "        \"\"\"Generates the experimental layer from the PDF\"\"\"\n",
    "\n",
    "        if len(self.dataset_xsizes) > 1:\n",
    "            splitting_layer = op.as_layer(\n",
    "                op.split,\n",
    "                op_args=[self.dataset_xsizes],\n",
    "                op_kwargs={\"axis\": 1},\n",
    "                name=f\"{self.name}_split\",\n",
    "            )\n",
    "            split_pdf = splitting_layer(pdf)\n",
    "        else: split_pdf = [pdf]\n",
    "        \n",
    "        output_layers = []\n",
    "        for p_pdf, obs, mask in zip(split_pdf, self.observables, self.masks):\n",
    "            if mask is not None:\n",
    "                output_layers.append(mask(obs(p_pdf)))\n",
    "            else:\n",
    "                output_layers.append(obs(p_pdf))\n",
    "        ret = op.concatenate(output_layers, axis=2)\n",
    "        if self.rotation is not None: ret = self.rotation(ret)\n",
    "        return ret\n",
    "\n",
    "    def __call__(self, pdf_layer, mask=None):\n",
    "        loss_f = self._generate_loss(mask)\n",
    "        experiment_prediction = self._generate_experimental_layer(pdf_layer)\n",
    "        return loss_f(experiment_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa408a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n3fit.layers import Mask\n",
    "\n",
    "def observable_generator(\n",
    "    spec_dict, positivity_initial=1.0, integrability=False\n",
    "):\n",
    "    \n",
    "    spec_name = spec_dict[\"name\"]\n",
    "    dataset_xsizes = []\n",
    "    model_inputs = []\n",
    "    model_obs_layers = []\n",
    "    tr_mask_layers = []\n",
    "    vl_mask_layers = []\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    obs_layer_tr, obs_layer_vl, obs_layer_ex = None, None, None\n",
    "    mask_tr, mask_vl  = None, None\n",
    "    \n",
    "    apply_masks = spec_dict.get(\"data_transformation_tr\") is None\n",
    "    \n",
    "    # The first step is to compute the observable for each of the datasets\n",
    "    for dataset_dict in spec_dict[\"datasets\"]:\n",
    "        # Get the generic information of the dataset\n",
    "        dataset_name = dataset_dict[\"name\"]\n",
    "        trmask = spec_dict[\"trmask\"][offset:offset + dataset_dict[\"ndata\"]]\n",
    "        tr_mask_layers.append(Mask(trmask, axis=2) if apply_masks else None)\n",
    "        vl_mask_layers.append(Mask(~trmask, axis=2) if apply_masks else None)\n",
    "\n",
    "        # Instantiate the DIS Observable \n",
    "        obs_layer = DIS(dataset_dict[\"fktables\"],\n",
    "                        dataset_dict[\"operation\"],\n",
    "                        name=f\"dat_{dataset_name}\")\n",
    "        model_obs_layers.append(obs_layer)\n",
    "\n",
    "        if obs_layer.splitting is None:\n",
    "            xgrid = dataset_dict[\"fktables\"][0][\"xgrid\"]\n",
    "            model_inputs.append(xgrid)\n",
    "            dataset_xsizes.append(xgrid.shape[1])\n",
    "        else:\n",
    "            xgrids = [i[\"xgrid\"] for i in dataset_dict[\"fktables\"]]\n",
    "            model_inputs += xgrids\n",
    "            dataset_xsizes.append(sum([i.shape[1] for i in xgrids]))\n",
    "        \n",
    "        # shift offset for new mask array\n",
    "        offset = offset + dataset_dict[\"ndata\"]\n",
    "\n",
    "    full_nx = sum(dataset_xsizes)\n",
    "    \n",
    "    if spec_dict[\"positivity\"]:\n",
    "        out_positivity = ObservableWrapper(\n",
    "            spec_name,\n",
    "            model_obs_layers,\n",
    "            tr_mask_layers,\n",
    "            dataset_xsizes,\n",
    "            multiplier=positivity_initial,\n",
    "            positivity=not integrability,\n",
    "            integrability=integrability,\n",
    "        )\n",
    "\n",
    "        layer_info = {\n",
    "            \"inputs\": model_inputs,\n",
    "            \"output_tr\": out_positivity,\n",
    "            \"experiment_xsize\": full_nx,\n",
    "        }\n",
    "        # For positivity we end here\n",
    "        return layer_info\n",
    "\n",
    "    if spec_dict.get(\"data_transformation_tr\") is not None:\n",
    "        obsrot_tr = ObsRotation(spec_dict.get(\"data_transformation_tr\"))\n",
    "        obsrot_vl = ObsRotation(spec_dict.get(\"data_transformation_vl\"))\n",
    "    else:\n",
    "        obsrot_tr = None\n",
    "        obsrot_vl = None\n",
    "\n",
    "    #console.log(\"expdata:\",spec_dict[\"expdata\"].shape)\n",
    "    #console.log(\"expdata_vl:\",spec_dict[\"expdata_vl\"].shape)\n",
    "    #console.log(\"expdata_true:\",spec_dict[\"expdata_true\"].shape)\n",
    "    \n",
    "    out_tr = ObservableWrapper(\n",
    "        spec_name,\n",
    "        model_obs_layers,\n",
    "        tr_mask_layers,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat\"],\n",
    "        data=spec_dict[\"expdata\"],\n",
    "        rotation=obsrot_tr,\n",
    "    )\n",
    "    out_vl = ObservableWrapper(\n",
    "        f\"{spec_name}_val\",\n",
    "        model_obs_layers,\n",
    "        vl_mask_layers,\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat_vl\"],\n",
    "        data=spec_dict[\"expdata_vl\"],\n",
    "        rotation=obsrot_vl,\n",
    "    )\n",
    "    out_exp = ObservableWrapper(\n",
    "        f\"{spec_name}_exp\",\n",
    "        model_obs_layers,\n",
    "        [None] * len(model_obs_layers),\n",
    "        dataset_xsizes,\n",
    "        invcovmat=spec_dict[\"invcovmat_true\"],\n",
    "        covmat=spec_dict[\"covmat\"],\n",
    "        data=spec_dict[\"expdata_true\"],\n",
    "        rotation=None,\n",
    "    )\n",
    "\n",
    "    layer_info = {\n",
    "        \"inputs\": model_inputs,\n",
    "        \"output\": out_exp,\n",
    "        \"output_tr\": out_tr,\n",
    "        \"output_vl\": out_vl,\n",
    "        \"experiment_xsize\": full_nx,\n",
    "    }\n",
    "    return layer_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2210c23",
   "metadata": {},
   "source": [
    "## 5. Compile the Models & Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc530bb",
   "metadata": {},
   "source": [
    "What remains to do now is to combined everything and construct a class that controls and performs a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e7ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "def _pdf_injection(pdf_layers, observables, masks):\n",
    "    \"\"\"Takes as input a list of PDF layers and if needed applies masks.\"\"\"\n",
    "    return [f(x, mask=m) for f, x, m in zip_longest(observables, pdf_layers, masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39b5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSH_POSITIVITY_EACH = 100\n",
    "PUSH_INTEGRABILITY_EACH = 100\n",
    "CHI2_THRESHOLD = 10.0\n",
    "\n",
    "def _LM_initial_and_multiplier(input_initial, input_multiplier, max_lambda, steps):\n",
    "    initial, multiplier = input_initial, input_multiplier\n",
    "    if multiplier is None:\n",
    "        if initial is None: initial = 1.0\n",
    "        multiplier = pow(max_lambda / initial, 1 / max(steps, 1))\n",
    "    elif initial is None:\n",
    "        initial = max_lambda / pow(multiplier, steps)\n",
    "    return initial, multiplier\n",
    "\n",
    "class ModelTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        exp_info,\n",
    "        pos_info,\n",
    "        integ_info,\n",
    "        flavinfo,\n",
    "        fitbasis,\n",
    "        nnseeds,\n",
    "        pass_status=\"ok\",\n",
    "        failed_status=\"fail\",\n",
    "        debug=False,\n",
    "        kfold_parameters=None,\n",
    "        max_cores=None,\n",
    "        model_file=None,\n",
    "        sum_rules=None,\n",
    "        parallel_models=1,\n",
    "    ):\n",
    "        \n",
    "        self.exp_info = exp_info\n",
    "        self.pos_info = pos_info\n",
    "        self.integ_info = integ_info\n",
    "        if self.integ_info is not None:\n",
    "            self.all_info = exp_info + pos_info + integ_info\n",
    "        else:\n",
    "            self.all_info = exp_info + pos_info\n",
    "        self.flavinfo = flavinfo\n",
    "        self.fitbasis = fitbasis\n",
    "        self._nn_seeds = nnseeds\n",
    "        self.pass_status = pass_status\n",
    "        self.failed_status = failed_status\n",
    "        self.debug = debug\n",
    "        self.all_datasets = []\n",
    "        self._scaler = None\n",
    "        self._parallel_models = parallel_models\n",
    "        self.impose_sumrule = sum_rules\n",
    "        self.kpartitions = [None]\n",
    "\n",
    "        self.input_list = []\n",
    "        self.input_sizes = []\n",
    "        self.training = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"posdatasets\": [],\n",
    "            \"posmultipliers\": [],\n",
    "            \"posinitials\": [],\n",
    "            \"integdatasets\": [],\n",
    "            \"integmultipliers\": [],\n",
    "            \"integinitials\": [],\n",
    "            \"folds\": [],\n",
    "        }\n",
    "        self.validation = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"folds\": [],\n",
    "            \"posdatasets\": [],\n",
    "        }\n",
    "        self.experimental = {\n",
    "            \"output\": [],\n",
    "            \"expdata\": [],\n",
    "            \"ndata\": 0,\n",
    "            \"model\": None,\n",
    "            \"folds\": [],\n",
    "        }\n",
    "\n",
    "        self._fill_the_dictionaries()\n",
    "\n",
    "        if self.validation[\"ndata\"] == 0:\n",
    "            self.no_validation = True\n",
    "            self.validation[\"expdata\"] = self.training[\"expdata\"]\n",
    "        else:\n",
    "            self.no_validation = False\n",
    "            \n",
    "        self.callbacks = [callbacks.TimerCallback()]\n",
    "            \n",
    "\n",
    "    def _model_generation(self, pdf_models, partition, partition_idx):\n",
    "       \n",
    "        log.info(\"Generating the Model\")\n",
    "\n",
    "        # Construct the input array that will be given to the pdf\n",
    "        input_arr = np.concatenate(self.input_list, axis=1).T\n",
    "        input_layer = op.numpy_to_input(input_arr)\n",
    "\n",
    "        all_replicas_pdf = []\n",
    "        for pdf_model in pdf_models:\n",
    "            full_model_input_dict, full_pdf = pdf_model.apply_as_layer([input_layer])\n",
    "            all_replicas_pdf.append(full_pdf)\n",
    "\n",
    "        full_pdf_per_replica = op.stack(all_replicas_pdf, axis=-1)\n",
    "\n",
    "        splitted_pdf = op.as_layer(\n",
    "            op.split, \n",
    "            op_args=[self.input_sizes], \n",
    "            op_kwargs={\"axis\": 1}, \n",
    "            name=\"pdf_split\"\n",
    "        )(full_pdf_per_replica)\n",
    "\n",
    "        training_mask = validation_mask = experimental_mask = [None]\n",
    "\n",
    "        output_tr = _pdf_injection(splitted_pdf, self.training[\"output\"], training_mask)\n",
    "        training = MetaModel(full_model_input_dict, output_tr)\n",
    "\n",
    "        val_pdfs, exp_pdfs = [], []\n",
    "        for partial_pdf, obs in zip(splitted_pdf, self.training[\"output\"]):\n",
    "            if not obs.positivity and not obs.integrability:\n",
    "                val_pdfs.append(partial_pdf)\n",
    "                exp_pdfs.append(partial_pdf)\n",
    "            elif not obs.integrability and obs.positivity:\n",
    "                val_pdfs.append(partial_pdf)\n",
    "\n",
    "        # We don't want to included the integrablity in the validation\n",
    "        output_vl = _pdf_injection(val_pdfs, self.validation[\"output\"], validation_mask)\n",
    "        validation = MetaModel(full_model_input_dict, output_vl)\n",
    "        output_ex = _pdf_injection(exp_pdfs, self.experimental[\"output\"], experimental_mask)\n",
    "        experimental = MetaModel(full_model_input_dict, output_ex)\n",
    "\n",
    "        # exp_model = MetaModel(full_model_input_dict, output_ex)\n",
    "        # vl_model = exp_model.append(vl_mask_layer)\n",
    "        # tr_model  = \n",
    "        \n",
    "        training.summary()\n",
    "\n",
    "        return {\n",
    "            \"training\": training,\n",
    "            \"validation\": validation,\n",
    "            \"experimental\": experimental,\n",
    "        }\n",
    "\n",
    "\n",
    "    def _generate_observables(\n",
    "        self,\n",
    "        all_pos_multiplier,\n",
    "        all_pos_initial,\n",
    "        all_integ_multiplier,\n",
    "        all_integ_initial,\n",
    "        epochs,\n",
    "        interpolation_points,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This functions fills the 3 dictionaries (training, validation, experimental)\n",
    "        with the output layers and the loss functions\n",
    "        It also fill the list of input tensors (input_list)\n",
    "\n",
    "        The arguments of this function are used to define the initial positivity of the\n",
    "        positivity observables and the multiplier to be applied at each step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            all_pos_multiplier: float, None\n",
    "                multiplier to be applied to the positivity each ``PUSH_POSITIVITY_EACH`` epochs\n",
    "            all_pos_initial: float, None\n",
    "                initial value for the positivity lambda\n",
    "            epochs: int\n",
    "                total number of epochs for the run\n",
    "        \"\"\"\n",
    "\n",
    "        # First reset the dictionaries\n",
    "        self._reset_observables()\n",
    "        console.print(\"Generating layers\", style=\"bold red\")\n",
    "        \n",
    "        for exp_dict in self.exp_info:\n",
    "            exp_layer = observable_generator(exp_dict)\n",
    "\n",
    "            self.input_list += exp_layer[\"inputs\"]\n",
    "            self.input_sizes.append(exp_layer[\"experiment_xsize\"])\n",
    "\n",
    "            self.training[\"output\"].append(exp_layer[\"output_tr\"])\n",
    "            self.validation[\"output\"].append(exp_layer[\"output_vl\"])\n",
    "            self.experimental[\"output\"].append(exp_layer[\"output\"])\n",
    "\n",
    "        for pos_dict in self.pos_info:\n",
    "            positivity_steps = int(epochs / PUSH_POSITIVITY_EACH)\n",
    "            max_lambda = pos_dict[\"lambda\"]\n",
    "\n",
    "            pos_initial, pos_multiplier = _LM_initial_and_multiplier(\n",
    "                all_pos_initial, \n",
    "                all_pos_multiplier, \n",
    "                max_lambda, \n",
    "                positivity_steps\n",
    "            )\n",
    "\n",
    "            pos_layer = observable_generator(pos_dict, positivity_initial=pos_initial)\n",
    "            self.input_list += pos_layer[\"inputs\"]\n",
    "            self.input_sizes.append(pos_layer[\"experiment_xsize\"])\n",
    "\n",
    "            self.training[\"output\"].append(pos_layer[\"output_tr\"])\n",
    "            self.validation[\"output\"].append(pos_layer[\"output_tr\"])\n",
    "            self.training[\"posmultipliers\"].append(pos_multiplier)\n",
    "            self.training[\"posinitials\"].append(pos_initial)\n",
    "\n",
    "        if self.integ_info is not None:\n",
    "            for integ_dict in self.integ_info:\n",
    "                integrability_steps = int(epochs / PUSH_INTEGRABILITY_EACH)\n",
    "                max_lambda = integ_dict[\"lambda\"]\n",
    "\n",
    "                integ_initial, integ_multiplier = _LM_initial_and_multiplier(\n",
    "                    all_integ_initial, \n",
    "                    all_integ_multiplier, \n",
    "                    max_lambda, \n",
    "                    integrability_steps\n",
    "                )\n",
    "\n",
    "                integ_layer = observable_generator(\n",
    "                    integ_dict, \n",
    "                    positivity_initial=integ_initial, \n",
    "                    integrability=True\n",
    "                )\n",
    "                self.input_list += integ_layer[\"inputs\"]\n",
    "                self.input_sizes.append(integ_layer[\"experiment_xsize\"])\n",
    "                self.training[\"output\"].append(integ_layer[\"output_tr\"])\n",
    "                self.training[\"integmultipliers\"].append(integ_multiplier)\n",
    "                self.training[\"integinitials\"].append(integ_initial)\n",
    "\n",
    "\n",
    "    def _generate_pdf(\n",
    "        self,\n",
    "        nodes_per_layer,\n",
    "        activation_per_layer,\n",
    "        initializer,\n",
    "        layer_type,\n",
    "        dropout,\n",
    "        regularizer,\n",
    "        regularizer_args,\n",
    "        seed,\n",
    "    ):\n",
    "        \"\"\"Generate the PDF NNs\"\"\"\n",
    "        \n",
    "        console.print(\"Generate PDF models\", style=\"bold red\")\n",
    "\n",
    "        pdf_models = pdfNN_layer_generator(\n",
    "            nodes=nodes_per_layer,\n",
    "            activations=activation_per_layer,\n",
    "            layer_type=layer_type,\n",
    "            flav_info=self.flavinfo,\n",
    "            fitbasis=self.fitbasis,\n",
    "            seed=seed,\n",
    "            initializer_name=initializer\n",
    "        )\n",
    "        return pdf_models\n",
    "    \n",
    "\n",
    "    def _train_and_fit(self, training_model, stopping_object, epochs=100):\n",
    "        \"\"\"\n",
    "        Trains the NN for the number of epochs given using\n",
    "        stopping_object as the stopping criteria\n",
    "\n",
    "        Every ``PUSH_POSITIVITY_EACH`` epochs the positivity will be multiplied by their\n",
    "        respective positivity multipliers.\n",
    "        In the same way, every ``PUSH_INTEGRABILITY_EACH`` epochs the integrability\n",
    "        will be multiplied by their respective integrability multipliers\n",
    "        \"\"\"\n",
    "        callback_st = callbacks.StoppingCallback(stopping_object)\n",
    "        callback_pos = callbacks.LagrangeCallback(\n",
    "            self.training[\"posdatasets\"],\n",
    "            self.training[\"posmultipliers\"],\n",
    "            update_freq=PUSH_POSITIVITY_EACH,\n",
    "        )\n",
    "        callback_integ = callbacks.LagrangeCallback(\n",
    "            self.training[\"integdatasets\"],\n",
    "            self.training[\"integmultipliers\"],\n",
    "            update_freq=PUSH_INTEGRABILITY_EACH,\n",
    "        )\n",
    "\n",
    "        training_model.perform_fit(\n",
    "            epochs=epochs,\n",
    "            verbose=False,\n",
    "            callbacks=self.callbacks + [callback_st, callback_pos, callback_integ],\n",
    "        )\n",
    "\n",
    "        if any(bool(i) for i in stopping_object.e_best_chi2):\n",
    "            return self.pass_status\n",
    "        return self.failed_status\n",
    "\n",
    "    def evaluate(self, stopping_object):\n",
    "        \"\"\"Returns the training, validation and experimental chi2\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            stopping_object\n",
    "                A Stopping intance which will have associated a validation model and the\n",
    "                list of output layers that should contribute to the training chi2\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            train_chi2: chi2 of the trainining set\n",
    "            val_chi2 : chi2 of the validation set\n",
    "            exp_chi2: chi2 of the experimental data (without replica or tr/vl split)\n",
    "        \"\"\"\n",
    "        if self.training[\"model\"] is None:\n",
    "            raise RuntimeError(\"Modeltrainer.evaluate was called before any training\")\n",
    "        train_chi2 = stopping_object.evaluate_training(self.training[\"model\"])\n",
    "        val_chi2 = stopping_object.vl_chi2\n",
    "        exp_chi2 = self.experimental[\"model\"].compute_losses()[\"loss\"] / self.experimental[\"ndata\"]\n",
    "        return train_chi2, val_chi2, exp_chi2\n",
    "\n",
    "    def hyperparametrizable(self, params):\n",
    "        \"\"\"\n",
    "        Wrapper around all the functions defining the fit.\n",
    "\n",
    "        After the ModelTrainer class has been instantiated,\n",
    "        a call to this function (with a ``params`` dictionary) is necessary\n",
    "        in order to generate the whole PDF model and perform a fit.\n",
    "\n",
    "        This is a necessary step for hyperopt to work\n",
    "\n",
    "        Parameters used only here:\n",
    "            - ``epochs``: maximum number of iterations for the fit to run\n",
    "            - ``stopping_patience``: patience of the stopper after finding a new minimum\n",
    "        All other parameters are passed to the corresponding functions\n",
    "        \"\"\"\n",
    "\n",
    "        clear_backend_state()\n",
    "\n",
    "        epochs = int(params[\"epochs\"])\n",
    "        stopping_patience = params[\"stopping_patience\"]\n",
    "        stopping_epochs = int(epochs * stopping_patience)\n",
    "\n",
    "        positivity_dict = params.get(\"positivity\", {})\n",
    "        integrability_dict = params.get(\"integrability\", {})\n",
    "        self._generate_observables(\n",
    "            positivity_dict.get(\"multiplier\"),\n",
    "            positivity_dict.get(\"initial\"),\n",
    "            integrability_dict.get(\"multiplier\"),\n",
    "            integrability_dict.get(\"initial\"),\n",
    "            epochs,\n",
    "            params.get(\"interpolation_points\"),\n",
    "        )\n",
    "        threshold_pos = positivity_dict.get(\"threshold\", 1e-6)\n",
    "        threshold_chi2 = params.get(\"threshold_chi2\", CHI2_THRESHOLD)\n",
    "\n",
    "        ### Training loop\n",
    "        for k, partition in enumerate(self.kpartitions):\n",
    "            seeds = self._nn_seeds\n",
    "\n",
    "            pdf_models = self._generate_pdf(\n",
    "                params[\"nodes_per_layer\"],\n",
    "                params[\"activation_per_layer\"],\n",
    "                params[\"initializer\"],\n",
    "                params[\"layer_type\"],\n",
    "                params[\"dropout\"],\n",
    "                params.get(\"regularizer\", None),\n",
    "                params.get(\"regularizer_args\", None),\n",
    "                seeds,\n",
    "            )\n",
    "\n",
    "            models = self._model_generation(pdf_models, partition, k)\n",
    "\n",
    "            reporting = self._prepare_reporting(partition)\n",
    "\n",
    "            if self.no_validation:\n",
    "                models[\"validation\"] = models[\"training\"]\n",
    "                validation_model = models[\"training\"]\n",
    "            else:\n",
    "                validation_model = models[\"validation\"]\n",
    "\n",
    "            stopping_object = Stopping(\n",
    "                validation_model,\n",
    "                reporting,\n",
    "                pdf_models,\n",
    "                total_epochs=epochs,\n",
    "                stopping_patience=stopping_epochs,\n",
    "                threshold_positivity=threshold_pos,\n",
    "                threshold_chi2=threshold_chi2,\n",
    "            )\n",
    "\n",
    "            # Compile each of the models with the right parameters\n",
    "            for model in models.values():\n",
    "                model.compile(**params[\"optimizer\"])\n",
    "\n",
    "            passed = self._train_and_fit(\n",
    "                models[\"training\"],\n",
    "                stopping_object,\n",
    "                epochs=epochs,\n",
    "            )\n",
    "\n",
    "        self.training[\"model\"] = models[\"training\"]\n",
    "        self.experimental[\"model\"] = models[\"experimental\"]\n",
    "        self.validation[\"model\"] = models[\"validation\"]\n",
    "\n",
    "        dict_out = {\"status\": passed, \"stopping_object\": stopping_object, \"pdf_models\": pdf_models}\n",
    "        return dict_out\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    #                   VARIOUS UTILS                  #\n",
    "    #--------------------------------------------------#\n",
    "    \n",
    "    def _reset_observables(self):\n",
    "        \"\"\"Reset the various lists.\"\"\"\n",
    "        \n",
    "        self.input_list = []\n",
    "        self.input_sizes = []\n",
    "        for key in [\"output\", \"posmultipliers\", \"integmultipliers\"]:\n",
    "            self.training[key] = []\n",
    "            self.validation[key] = []\n",
    "            self.experimental[key] = []\n",
    "            \n",
    "    \n",
    "    def _fill_the_dictionaries(self):\n",
    "        \"\"\"Transfer information from `self.exp_info` into the dictionaries.\"\"\"\n",
    "        \n",
    "        for exp_dict in self.exp_info:\n",
    "            self.training[\"expdata\"].append(exp_dict[\"expdata\"])\n",
    "            self.validation[\"expdata\"].append(exp_dict[\"expdata_vl\"])\n",
    "            self.experimental[\"expdata\"].append(exp_dict[\"expdata_true\"])\n",
    "\n",
    "            self.training[\"folds\"].append(exp_dict[\"folds\"][\"training\"])\n",
    "            self.validation[\"folds\"].append(exp_dict[\"folds\"][\"validation\"])\n",
    "            self.experimental[\"folds\"].append(exp_dict[\"folds\"][\"experimental\"])\n",
    "\n",
    "            nd_tr = exp_dict[\"ndata\"]\n",
    "            nd_vl = exp_dict[\"ndata_vl\"]\n",
    "\n",
    "            self.training[\"ndata\"] += nd_tr\n",
    "            self.validation[\"ndata\"] += nd_vl\n",
    "            self.experimental[\"ndata\"] += nd_tr + nd_vl\n",
    "\n",
    "            for dataset in exp_dict[\"datasets\"]:\n",
    "                self.all_datasets.append(dataset[\"name\"])\n",
    "        self.all_datasets = set(self.all_datasets)\n",
    "\n",
    "        for pos_dict in self.pos_info:\n",
    "            self.training[\"expdata\"].append(pos_dict[\"expdata\"])\n",
    "            self.training[\"posdatasets\"].append(pos_dict[\"name\"])\n",
    "            self.validation[\"expdata\"].append(pos_dict[\"expdata\"])\n",
    "            self.validation[\"posdatasets\"].append(pos_dict[\"name\"])\n",
    "        if self.integ_info is not None:\n",
    "            for integ_dict in self.integ_info:\n",
    "                self.training[\"expdata\"].append(integ_dict[\"expdata\"])\n",
    "                self.training[\"integdatasets\"].append(integ_dict[\"name\"])\n",
    "                \n",
    "    def _prepare_reporting(self, partition):\n",
    "        \"\"\"Parses the information received by the :py:class:`n3fit.ModelTrainer.ModelTrainer`\n",
    "        to select the bits necessary for reporting the chi2.\n",
    "        Receives the chi2 partition data to see whether any dataset is to be left out\n",
    "        \"\"\"\n",
    "        reported_keys = [\"name\", \"count_chi2\", \"positivity\", \"integrability\", \"ndata\", \"ndata_vl\"]\n",
    "        reporting_list = []\n",
    "        for exp_dict in self.all_info:\n",
    "            reporting_dict = {k: exp_dict.get(k) for k in reported_keys}\n",
    "            reporting_list.append(reporting_dict)\n",
    "        return reporting_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f763fcb",
   "metadata": {},
   "source": [
    "## 6. Peform Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69f407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnseed = [1872583848] * 5\n",
    "fitbasis = 'EVOL'\n",
    "model_file = None\n",
    "\n",
    "flav_info = [\n",
    "    {'fl': 'sng', 'trainable': False, 'smallx': [1.094, 1.118], 'largex': [1.46, 3.003]}, \n",
    "    {'fl': 'g', 'trainable': False, 'smallx': [0.8189, 1.044], 'largex': [2.791, 5.697]}, \n",
    "    {'fl': 'v', 'trainable': False, 'smallx': [0.457, 0.7326], 'largex': [1.56, 3.431]}, \n",
    "    {'fl': 'v3', 'trainable': False, 'smallx': [0.1462, 0.4061], 'largex': [1.745, 3.452]}, \n",
    "    {'fl': 'v8', 'trainable': False, 'smallx': [0.5401, 0.7665], 'largex': [1.539, 3.393]}, \n",
    "    {'fl': 't3', 'trainable': False, 'smallx': [-0.4401, 0.9163], 'largex': [1.773, 3.333]}, \n",
    "    {'fl': 't8', 'trainable': False, 'smallx': [0.5852, 0.8537], 'largex': [1.533, 3.436]}, \n",
    "    {'fl': 't15', 'trainable': False, 'smallx': [1.082, 1.142], 'largex': [1.461, 3.1]}\n",
    "]\n",
    "\n",
    "params = {\n",
    "    'nodes_per_layer': [15, 10, 14], \n",
    "    'activation_per_layer': ['sigmoid', 'sigmoid', 'linear'], \n",
    "    'initializer': 'glorot_normal', \n",
    "    'optimizer': {'optimizer_name': 'RMSprop', 'learning_rate': 0.01, 'clipnorm': 1.0}, \n",
    "    'epochs': 900, 'positivity': {'multiplier': 1.05, 'initial': None, 'threshold': 1e-05}, \n",
    "    'stopping_patience': 0.3, 'layer_type': 'dense', 'dropout': 0.0, 'threshold_chi2': 5.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aec040a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Generating layers</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mGenerating layers\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Generate PDF models</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mGenerate PDF models\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:01:46] </span>The incovmat input data is now                                                          <a href=\"file:///tmp/ipykernel_22307/1194756364.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194756364.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_22307/1194756364.py#22\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">76</span>,<span style=\"font-weight: bold\">)</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[14:01:46]\u001b[0m\u001b[2;36m \u001b[0mThe incovmat input data is now                                                          \u001b]8;id=767598;file:///tmp/ipykernel_22307/1194756364.py\u001b\\\u001b[2m1194756364.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=470099;file:///tmp/ipykernel_22307/1194756364.py#22\u001b\\\u001b[2m22\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m76\u001b[0m,\u001b[1m)\u001b[0m                                                                                   \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>The input trvl mask is now <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">121</span>,<span style=\"font-weight: bold\">)</span>                                                       <a href=\"file:///tmp/ipykernel_22307/1194756364.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194756364.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_22307/1194756364.py#24\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mThe input trvl mask is now \u001b[1m(\u001b[0m\u001b[1;36m121\u001b[0m,\u001b[1m)\u001b[0m                                                       \u001b]8;id=699545;file:///tmp/ipykernel_22307/1194756364.py\u001b\\\u001b[2m1194756364.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=149081;file:///tmp/ipykernel_22307/1194756364.py#24\u001b\\\u001b[2m24\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:01:47] </span>The incovmat input data is now                                                          <a href=\"file:///tmp/ipykernel_22307/1194756364.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194756364.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_22307/1194756364.py#22\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>,<span style=\"font-weight: bold\">)</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[14:01:47]\u001b[0m\u001b[2;36m \u001b[0mThe incovmat input data is now                                                          \u001b]8;id=305976;file:///tmp/ipykernel_22307/1194756364.py\u001b\\\u001b[2m1194756364.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=955410;file:///tmp/ipykernel_22307/1194756364.py#22\u001b\\\u001b[2m22\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m78\u001b[0m,\u001b[1m)\u001b[0m                                                                                   \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>The input trvl mask is now <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">121</span>,<span style=\"font-weight: bold\">)</span>                                                       <a href=\"file:///tmp/ipykernel_22307/1194756364.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194756364.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_22307/1194756364.py#24\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mThe input trvl mask is now \u001b[1m(\u001b[0m\u001b[1;36m121\u001b[0m,\u001b[1m)\u001b[0m                                                       \u001b]8;id=492089;file:///tmp/ipykernel_22307/1194756364.py\u001b\\\u001b[2m1194756364.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=878536;file:///tmp/ipykernel_22307/1194756364.py#24\u001b\\\u001b[2m24\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>The incovmat input data is now                                                          <a href=\"file:///tmp/ipykernel_22307/1194756364.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194756364.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_22307/1194756364.py#22\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">154</span>,<span style=\"font-weight: bold\">)</span>                                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mThe incovmat input data is now                                                          \u001b]8;id=547010;file:///tmp/ipykernel_22307/1194756364.py\u001b\\\u001b[2m1194756364.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=840430;file:///tmp/ipykernel_22307/1194756364.py#22\u001b\\\u001b[2m22\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m154\u001b[0m,\u001b[1m)\u001b[0m                                                                                  \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"meta_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(1, 290, 1)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "PDF_0 (MetaModel)               (1, None, 14)        359         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "PDF_1 (MetaModel)               (1, None, 14)        359         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "PDF_2 (MetaModel)               (1, None, 14)        359         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "PDF_3 (MetaModel)               (1, None, 14)        359         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "PDF_4 (MetaModel)               (1, None, 14)        359         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(1, 290, 14, 5)]    0           PDF_0[0][0]                      \n",
      "                                                                 PDF_1[0][0]                      \n",
      "                                                                 PDF_2[0][0]                      \n",
      "                                                                 PDF_3[0][0]                      \n",
      "                                                                 PDF_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pdf_split (Lambda)              [(1, 100, 14, 5), (1 0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "DEUTERON_split (Lambda)         [(1, 50, 14, 5), (1, 0           pdf_split[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_NMCPD_dw (DIS)              (1, 5, 121)          0           DEUTERON_split[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dat_SLACP_dwsh (DIS)            (1, 5, 33)           0           DEUTERON_split[0][1]             \n",
      "__________________________________________________________________________________________________\n",
      "dat_POSF2U (DIS)                (1, 5, 20)           0           pdf_split[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_POSFLL (DIS)                (1, 5, 20)           0           pdf_split[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "dat_INTEGXT3 (DIS)              (1, 5, 1)            0           pdf_split[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "mask (Mask)                     (1, 5, None)         0           dat_NMCPD_dw[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask_2 (Mask)                   (1, 5, None)         0           dat_SLACP_dwsh[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mask_4 (Mask)                   (1, 5, None)         0           dat_POSF2U[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mask_6 (Mask)                   (1, 5, None)         0           dat_POSFLL[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mask_8 (Mask)                   (1, 5, None)         0           dat_INTEGXT3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ConcatV2 (TensorFlo [(1, 5, None)]       0           mask[0][0]                       \n",
      "                                                                 mask_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity (TensorFlo [(1, 5, None)]       0           mask_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_1 (TensorF [(1, 5, None)]       0           mask_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_2 (TensorF [(1, 5, None)]       0           mask_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "DEUTERON (LossInvcovmat)        (5,)                 0           tf_op_layer_ConcatV2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "POSF2U (LossPositivity)         (5,)                 1           tf_op_layer_Identity[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "POSFLL (LossPositivity)         (5,)                 1           tf_op_layer_Identity_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "INTEGXT3 (LossIntegrability)    (5,)                 1           tf_op_layer_Identity_2[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 1,798\n",
      "Trainable params: 1,795\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ModelTraining = ModelTrainer(\n",
    "    toyexpinfo,\n",
    "    toyposdatasets,\n",
    "    toyintegdatasets,\n",
    "    flav_info,\n",
    "    fitbasis,\n",
    "    nnseed\n",
    ")\n",
    "\n",
    "results = ModelTraining.hyperparametrizable(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e3d5e",
   "metadata": {},
   "source": [
    "## 7. Tasks\n",
    "\n",
    "### 7.1 Flavour Map & Convolution\n",
    "\n",
    "Recall that the convolution (matri multplication) of the the PDFs with the FK tables are computed as follows:\n",
    "$$ \\mathcal{O}^{\\rm th}_d = \\sum^{N_x}_{\\alpha} \\sum^{N_f}_{i} \\mathrm{FK}_{\\alpha i}^{(d)} \\mathrm{NN}_i (x_\\alpha, Q_0), $$\n",
    "Since for a given dataset/process, not all the flavours are active, therefore some of the flavours are pre-set to zero in the FK tables and are used to skip some parts of the computation.\n",
    "\n",
    "Understand how this flavour map works and how this gets propagated in the code. You might want to look more closely at the `fktable_dicts` entry in the class `Observable`. For a very brief detailed on how FK tables are structured, have a look at this [section](https://docs.nnpdf.science/data/th-data-files.html?highlight=flavourmap#fk-table-compression).\n",
    "\n",
    "### 7.2 Towards fitting structure functions\n",
    "\n",
    "In order to fit structure functions, the observable that we are interested in is generally the double differential cross section expressed as:\n",
    "\n",
    "$$ \\frac{d^2 \\sigma^2_{\\nu N}}{dx dQ^2} = f(Q^2, m_W^2) [ Y_+ F_2^{\\nu N} (x,Q^2) + Y_- F_3^{\\nu N} (x,Q^2) - y^2 F_L^{\\nu N} (x,Q^2) ]. $$\n",
    "\n",
    "How would the FK tables and the flavour map change in this scenario? Think of ways in which one can adapt the FK tables and flavour map concepts to take the above into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf",
   "language": "python",
   "name": "nnpdf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
